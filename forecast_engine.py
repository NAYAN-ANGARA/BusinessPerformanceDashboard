"""
forecast_engine.py
==================
Self-contained forecasting module for the Marketplace Business Insights dashboard.

All heavy sklearn imports live HERE — not in app.py.
The module loads pre-built model templates from `models.pkl` (generated by
train_model.py) using Streamlit's @st.cache_resource so the pickle is read
only once per server session, keeping memory low and cold-start fast.

Public API
----------
    build_features(dates_series, y_hist_series=None, origin_date=None) -> pd.DataFrame
    ensemble_forecast(dates_hist, y_hist, dates_future, use_yoy_data=None)
        -> (blended_preds, pred_std, confidence_pct, weighted_r2, model_detail)
"""

from __future__ import annotations

import copy
import pickle
import warnings
import os

import numpy as np
import pandas as pd
import streamlit as st

# ---------------------------------------------------------------------------
# sklearn imports — all in one place, never in app.py
# ---------------------------------------------------------------------------
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics         import r2_score

warnings.filterwarnings("ignore")

# ---------------------------------------------------------------------------
# Model loader — cached so the pickle is read only ONCE per Streamlit session
# ---------------------------------------------------------------------------
MODELS_PKL_PATH = os.path.join(os.path.dirname(__file__), "models.pkl")


@st.cache_resource(show_spinner=False)
def _load_model_templates() -> dict:
    """
    Load unfitted model templates from models.pkl.
    Cached by Streamlit so the file is read once per server session.

    If models.pkl is missing, falls back to building templates on-the-fly
    (requires sklearn) so the app still works without the pickle.
    """
    if os.path.exists(MODELS_PKL_PATH):
        with open(MODELS_PKL_PATH, "rb") as f:
            templates = pickle.load(f)
        return templates

    # --- Fallback: build in-memory (slower, but safe) ---
    st.warning(
        "⚠️ `models.pkl` not found — building model templates on-the-fly. "
        "Run `python train_model.py` once to generate the pickle and speed up forecasting."
    )
    from sklearn.linear_model  import Ridge, HuberRegressor
    from sklearn.ensemble      import (GradientBoostingRegressor,
                                       RandomForestRegressor, ExtraTreesRegressor)
    from sklearn.preprocessing import PolynomialFeatures, RobustScaler
    from sklearn.pipeline      import Pipeline

    return {
        "Gradient Boosting": GradientBoostingRegressor(
            n_estimators=300, max_depth=4, learning_rate=0.03,
            subsample=0.85, min_samples_leaf=3,
            validation_fraction=0.1, n_iter_no_change=20, random_state=42,
        ),
        "Extra Trees": ExtraTreesRegressor(
            n_estimators=300, max_depth=8, min_samples_leaf=2,
            random_state=42, n_jobs=-1,
        ),
        "Random Forest": RandomForestRegressor(
            n_estimators=300, max_depth=8, min_samples_leaf=2,
            random_state=42, n_jobs=-1,
        ),
        "Ridge Poly-3": Pipeline([
            ("poly",   PolynomialFeatures(degree=3, include_bias=False)),
            ("scaler", RobustScaler()),
            ("ridge",  Ridge(alpha=5.0)),
        ]),
        "Ridge Poly-2": Pipeline([
            ("poly",   PolynomialFeatures(degree=2, include_bias=False)),
            ("scaler", RobustScaler()),
            ("ridge",  Ridge(alpha=0.5)),
        ]),
        "Huber Regression": Pipeline([
            ("scaler", RobustScaler()),
            ("huber",  HuberRegressor(epsilon=1.35, max_iter=300)),
        ]),
    }


# ---------------------------------------------------------------------------
# Feature engineering
# ---------------------------------------------------------------------------

def build_features(
    dates_series,
    y_hist_series=None,
    origin_date=None,
) -> pd.DataFrame:
    """
    Build a 30-feature DataFrame for e-commerce time-series forecasting.

    Parameters
    ----------
    dates_series   : array-like of datetime — the dates to featurise
    y_hist_series  : array-like of float, optional — historical target values
                     used to compute lag / rolling features.
                     Pass None for future dates (lags will be filled with 0).
    origin_date    : datetime, optional — reference date for the 'days' counter.
                     Defaults to min(dates_series).

    Returns
    -------
    pd.DataFrame with shape (len(dates_series), ~30)
    """
    # Guarantee a proper Series so .dt accessor always works
    if not isinstance(dates_series, pd.Series):
        dates_series = pd.Series(dates_series)
    dates_series = dates_series.reset_index(drop=True)

    if origin_date is None:
        origin_date = dates_series.min()

    n = len(dates_series)
    df_feat = pd.DataFrame(index=range(n))

    # ── Calendar features ────────────────────────────────────────────────
    df_feat["days"]     = (dates_series - origin_date).dt.days.values
    df_feat["dow"]      = dates_series.dt.dayofweek.values
    df_feat["month"]    = dates_series.dt.month.values
    df_feat["quarter"]  = dates_series.dt.quarter.values
    df_feat["dom"]      = dates_series.dt.day.values
    df_feat["week"]     = dates_series.dt.isocalendar().week.astype(int).values
    df_feat["is_wkend"] = (dates_series.dt.dayofweek >= 5).astype(int).values
    df_feat["is_mon"]   = (dates_series.dt.dayofweek == 0).astype(int).values
    df_feat["is_fri"]   = (dates_series.dt.dayofweek == 4).astype(int).values
    df_feat["dom_norm"] = df_feat["dom"] / 31.0
    df_feat["days_sq"]  = df_feat["days"] ** 2  # quadratic trend

    # ── Cyclical encodings (prevent Dec→Jan large-jump artefact) ────────
    df_feat["dow_sin"]   = np.sin(2 * np.pi * df_feat["dow"]   / 7)
    df_feat["dow_cos"]   = np.cos(2 * np.pi * df_feat["dow"]   / 7)
    df_feat["month_sin"] = np.sin(2 * np.pi * df_feat["month"] / 12)
    df_feat["month_cos"] = np.cos(2 * np.pi * df_feat["month"] / 12)
    df_feat["week_sin"]  = np.sin(2 * np.pi * df_feat["week"]  / 52)
    df_feat["week_cos"]  = np.cos(2 * np.pi * df_feat["week"]  / 52)

    # ── Lag / rolling features ───────────────────────────────────────────
    if y_hist_series is not None:
        y_arr = np.array(y_hist_series, dtype=float)

        lag7   = np.full(n, np.nan)
        lag14  = np.full(n, np.nan)
        lag28  = np.full(n, np.nan)
        roll7  = np.full(n, np.nan)
        roll14 = np.full(n, np.nan)
        roll28 = np.full(n, np.nan)
        trend7 = np.full(n, np.nan)
        vol7   = np.full(n, np.nan)

        for i in range(n):
            if i >= 7:   lag7[i]   = y_arr[i - 7]
            if i >= 14:  lag14[i]  = y_arr[i - 14]
            if i >= 28:  lag28[i]  = y_arr[i - 28]
            if i >= 7:   roll7[i]  = np.mean(y_arr[max(0, i - 7):i])
            if i >= 14:  roll14[i] = np.mean(y_arr[max(0, i - 14):i])
            if i >= 28:  roll28[i] = np.mean(y_arr[max(0, i - 28):i])
            if i >= 14:
                m1 = np.mean(y_arr[max(0, i - 7):i])
                m2 = np.mean(y_arr[max(0, i - 14):max(0, i - 7)])
                trend7[i] = m1 - m2
            if i >= 7:
                vol7[i] = np.std(y_arr[max(0, i - 7):i]) + 1e-9

        # Fill NaNs with column mean (safe for tree-based models)
        for arr in [lag7, lag14, lag28, roll7, roll14, roll28, trend7, vol7]:
            mask = np.isnan(arr)
            arr[mask] = np.nanmean(arr) if not np.all(mask) else 0.0

        df_feat["lag7"]   = lag7
        df_feat["lag14"]  = lag14
        df_feat["lag28"]  = lag28
        df_feat["roll7"]  = roll7
        df_feat["roll14"] = roll14
        df_feat["roll28"] = roll28
        df_feat["trend7"] = trend7
        df_feat["vol7"]   = vol7
    else:
        # Future dates: lags unknown, fill with 0 (model discounts them)
        for col in ["lag7", "lag14", "lag28", "roll7", "roll14", "roll28", "trend7", "vol7"]:
            df_feat[col] = 0.0

    return df_feat


# ---------------------------------------------------------------------------
# Core ensemble engine
# ---------------------------------------------------------------------------

def ensemble_forecast(
    dates_hist,
    y_hist,
    dates_future,
    use_yoy_data=None,
):
    """
    Train 6 diverse models on historical data via walk-forward TimeSeriesSplit
    cross-validation (respects temporal order), then blend predictions using a
    softmax of R² scores so the best model on *your* data dominates.

    Parameters
    ----------
    dates_hist   : array-like of datetime — historical dates
    y_hist       : array-like of float   — historical target values
    dates_future : array-like of datetime — future dates to predict
    use_yoy_data : array-like of float, optional — same-period-last-year values
                   for a multiplicative seasonal adjustment on the blend

    Returns
    -------
    tuple:
        blended_preds : np.ndarray  — final blended forecast
        pred_std      : np.ndarray  — inter-model std (confidence band)
        confidence    : float       — 45–97 range confidence score
        weighted_r2   : float       — softmax-weighted mean cross-val R²
        model_detail  : dict        — per-model R² and blend weights
    """
    # Load model templates once (cached across Streamlit re-runs)
    model_templates = _load_model_templates()

    # --- Prepare arrays ---
    dates_hist = pd.Series(dates_hist) if not isinstance(dates_hist, pd.Series) else dates_hist
    y = np.array(y_hist, dtype=float)
    n = len(y)

    origin   = dates_hist.min()
    X_hist   = build_features(dates_hist,   y_hist_series=y, origin_date=origin).values
    X_future = build_features(dates_future, y_hist_series=None, origin_date=origin).values

    # --- Walk-forward TimeSeriesSplit CV + final fit ----------------------
    n_splits = min(5, max(2, n // 14))
    tscv = TimeSeriesSplit(
        n_splits=n_splits,
        test_size=max(3, n // (n_splits + 1)),
    )
    results: dict[str, dict] = {}

    for name, template in model_templates.items():
        fold_r2s = []
        try:
            for train_idx, val_idx in tscv.split(X_hist):
                X_tr, X_val = X_hist[train_idx], X_hist[val_idx]
                y_tr, y_val = y[train_idx],       y[val_idx]
                if len(y_tr) < 5:
                    continue
                # Deep-copy so the template stays unfitted for the next fold
                mdl_clone = copy.deepcopy(template)
                try:
                    mdl_clone.fit(X_tr, y_tr)
                    p = mdl_clone.predict(X_val)
                    fold_r2s.append(r2_score(y_val, p))
                except Exception:
                    fold_r2s.append(0.0)

            cv_r2 = float(np.clip(np.mean(fold_r2s) if fold_r2s else 0.0, 0.0, 1.0))

            # Final fit on ALL historical data
            mdl_final = copy.deepcopy(template)
            mdl_final.fit(X_hist, y)
            preds = np.maximum(mdl_final.predict(X_future), 0)
            results[name] = {"r2": cv_r2, "preds": preds}

        except Exception:
            results[name] = {
                "r2": 0.0,
                "preds": np.full(len(X_future), float(np.mean(y))),
            }

    # --- Holt's Double Exponential Smoothing (trend-aware, no sklearn) ---
    try:
        alpha, beta = 0.4, 0.2
        l_t = float(y[0])
        b_t = float(y[1] - y[0]) if n > 1 else 0.0
        for v in y[1:]:
            l_prev, b_prev = l_t, b_t
            l_t = alpha * v + (1 - alpha) * (l_prev + b_prev)
            b_t = beta * (l_t - l_prev) + (1 - beta) * b_prev

        holt_preds = np.maximum(
            np.array([l_t + (i + 1) * b_t for i in range(len(X_future))]), 0
        )
        eval_n = max(3, n // 5)
        y_eval = y[-eval_n:]
        h_eval = np.maximum(
            np.array([l_t + (i + 1) * b_t for i in range(eval_n)]), 0
        )
        holt_r2 = float(np.clip(r2_score(y_eval, h_eval[: len(y_eval)]), 0.0, 1.0))
        results["Holt Smoothing"] = {"r2": holt_r2, "preds": holt_preds}
    except Exception:
        pass

    if not results:
        fallback = np.full(len(X_future), float(np.mean(y)))
        return fallback, fallback * 0.1, 40.0, 0.0, {}

    # --- Softmax-weighted blend (high-R² models dominate) ----------------
    r2_vals    = np.array([r["r2"] for r in results.values()])
    exp_r2     = np.exp(r2_vals * 4)           # temperature = 4
    weights    = exp_r2 / exp_r2.sum()

    pred_matrix = np.vstack([r["preds"] for r in results.values()])
    blended     = np.maximum((pred_matrix * weights[:, None]).sum(axis=0), 0)

    # --- YoY seasonal multiplicative adjustment --------------------------
    if use_yoy_data is not None and len(use_yoy_data) > 0:
        yoy_arr    = np.array(use_yoy_data, dtype=float)
        yoy_avg    = np.mean(yoy_arr[yoy_arr > 0]) if np.any(yoy_arr > 0) else 0
        recent_avg = np.mean(y[-min(30, n):])
        if yoy_avg > 0 and recent_avg > 0:
            growth_factor = np.clip(recent_avg / yoy_avg, 0.4, 4.0)
            blended = blended * 0.60 + blended * growth_factor * 0.40

    # --- Inter-model std (confidence band) --------------------------------
    pred_std = pred_matrix.std(axis=0)

    # --- Composite confidence score (45–97 range) -------------------------
    weighted_r2  = float(np.dot(weights, r2_vals))
    data_bonus   = np.clip((n - 14) / 180, 0, 0.12)
    yoy_bonus    = 0.06 if (use_yoy_data is not None and len(use_yoy_data) > 0) else 0
    agree_bonus  = np.clip(1.0 - pred_std.mean() / (np.mean(y) + 1e-9), 0, 0.08)

    raw_conf   = weighted_r2 + data_bonus + yoy_bonus + agree_bonus
    confidence = float(np.clip(45 + raw_conf * 55, 45, 97))

    model_detail = {
        nm: {"r2": round(float(rv) * 100, 1)}
        for nm, rv in zip(results.keys(), r2_vals)
    }
    model_detail["_weights"] = {
        nm: round(float(w) * 100, 1)
        for nm, w in zip(results.keys(), weights)
    }

    return blended, pred_std, confidence, weighted_r2, model_detail